# DeepSeek_Code_Finetuning

### ν™κ²½
OS : Window
GPU : 3060 * 2 (12GB)
Lang : Python - 3.10.12
Lib : Transformers, Datasets, Torch, Trl
Datasets : code_search_net - Python

#### λ°©λ²•
DeepSeek-R1-Distill-Llama-8B λ¨λΈμ„ Loraμ™€ μ–‘μν™”λ¥Ό μ‚¬μ©ν•μ—¬ FineTuning

### νλΌλ―Έν„°
3060 * 2 μΌλ΅λ” μ–‘μν™” ν›„μ—λ„ 
* Max_length = 128
* per_device_batch_size = 8
* gradient_accumulation_steps = 2
* ν•™μµ μ‹ 1epochλ‹Ή 28μ‹κ°„ κ°€λ‰ μ†μ”

---

κ°™μ€ ν•λ“μ›¨μ–΄ ν™κ²½ λ°μ΄ν„° Docstring μ¶”κ°€ νλΌλ―Έν„° λ³€κ²½
3060 * 2 μΌλ΅λ” μ–‘μν™” ν›„μ—λ„ 
* Max_length = 512
* per_device_batch_size = 2
* gradient_accumulation_steps = 2
* ν•™μµ μ‹ 1epochλ‹Ή 122μ‹κ°„ κ°€λ‰ μ†μ”
* κ²°κ³Ό TrainOutput(global_step=103044, training_loss=1.2967534739197164, metrics={'train_runtime': 442653.9183, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.233, 'total_flos': 9.507085746142446e+18, 'train_loss': 1.2967534739197164, 'epoch': 0.9999951477274381})

### π€ λ¨λΈ ν•™μµ κ²°κ³Ό

#### π“ ν•™μµ κ°μ”  
- **μ΄ ν•™μµ μ¤ν…**: `103,044`  
- **μ΄ ν•™μµ μ‹κ°„**: `μ•½ 123μ‹κ°„ (442,653μ΄)`  
- **μ—ν¬ν¬(epoch)**: `1`  
- **μµμΆ… μ†μ‹¤ κ°’ (Loss)**: `1.2967`  

## π“ ν•™μµ μ§€ν‘  

| λ©”νΈλ¦­ | κ°’ | μ„¤λ… |
|--------|--------------------------|-----------------------------------------------|
| **global_step** | `103044` | μ΄ ν•™μµ μ¤ν… μ |
| **training_loss** | `1.2967` | μµμΆ… μ†μ‹¤ κ°’ (λ‚®μ„μλ΅ μ„±λ¥ ν–¥μƒ) |
| **train_runtime** | `442,653.9183 μ΄` | μ „μ²΄ ν•™μµ μ†μ” μ‹κ°„ (μ•½ 123μ‹κ°„) |
| **train_samples_per_second** | `0.931` | μ΄λ‹Ή μ²λ¦¬ν• μƒν” μ |
| **train_steps_per_second** | `0.233` | μ΄λ‹Ή μ²λ¦¬ν• μ¤ν… μ |
| **total_flos** | `9.51 Γ— 10ΒΉβΈ FLOPs` | μ΄ μ—°μ‚°λ‰ (FLOPs: Floating Point Operations) |
| **epoch** | `0.99999` | ν•™μµλ μ—ν¬ν¬ μ (β‰ 1 μ—ν¬ν¬) |

## π” λ¶„μ„ λ° κ°μ„  λ°©μ•  
- ν•™μµ μ†λ„κ°€ μƒλ€μ μΌλ΅ λλ¦¬λ―€λ΅, **λ°°μΉ ν¬κΈ° μ¦κ°€, λ°μ΄ν„° λ΅λ”© μµμ ν™”, GPU μ„±λ¥ ν–¥μƒ** λ“±μ„ κ³ λ ¤
- μ†μ‹¤ κ°’μ„ λ” λ‚®μ¶”λ ¤λ©΄, **ν•™μµλ¥  μ΅°μ •, λ°μ΄ν„° μ¦κ°•(Augmentation), λ¨λΈ κµ¬μ΅° κ°μ„ ** λ“±μ„ μ‹λ„ 
