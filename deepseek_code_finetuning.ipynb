{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "#from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.amp import autocast, GradScaler\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # ë©”ëª¨ë¦¬ ì¡°ê° ë°©ì§€ì§€\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_MODEL = \"./DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"code_search_net\", \"python\")  # ì–¸ì–´ ì„ íƒ ê°€ëŠ¥ (ex: python, java)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:784: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.62s/it]\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    #target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # ê°€ì¤‘ì¹˜ ì ìš©í•  ë ˆì´ì–´\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],  # ê°€ì¤‘ì¹˜ ì ìš©í•  ë ˆì´ì–´\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4bit ì–‘ìí™” ì„¤ì • - QLoRAë¡œ í•´ì•¼ í•¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # íŒ¨ë”© í† í° ì„¤ì •\n",
    "\n",
    "# 4-bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    #device_map=\"sequential\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # 4-bit ì„¤ì • ì ìš©\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn ì—ëŸ¬\n",
    "model.enable_input_require_grads() # get_input_embeddings().weight.requires_grad = True \n",
    "# LoRA ì ìš©\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# LoRAê°€ ì ìš©ëœ ë ˆì´ì–´ë§Œ í•™ìŠµ (ëª¨ë¸ íŒŒë¼ë¯¸í„° freeze)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23107/23107 [00:07<00:00, 2889.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # í•¨ìˆ˜ ì½”ë“œì™€ Docstringì„ í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ê²°í•©\n",
    "    combined_texts = [\n",
    "        f\"{doc}\\n\\n{code}\" for doc, code in zip(examples[\"func_documentation_string\"], examples[\"func_code_string\"])\n",
    "    ]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        combined_texts,  \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # âœ… `torch.tensor()` ì‚¬ìš©\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokenize_function(examples):\\n    tokenized = tokenizer(\\n        examples[\"func_code_string\"], \\n        truncation=True, \\n        padding=\"max_length\", \\n        max_length=128\\n    )\\n    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # âœ… `torch.tensor()` ì‚¬ìš©\\n    return tokenized\\n\\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"func_code_string\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # âœ… `torch.tensor()` ì‚¬ìš©\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "'''\n",
    "\n",
    "#tokenized_datasets.set_format(type=\"torch\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-code-doc-lora\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_checkpointing=False,  # T: GPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    gradient_accumulation_steps=2,  # ì‘ì€ ë°°ì¹˜ í¬ê¸° ë³´ì™„\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./code_logs\",\n",
    "    fp16=True,  # 16-bit ì—°ì‚°\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53770' max='103044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 53770/103044 62:24:21 < 57:11:24, 0.24 it/s, Epoch 0.52/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥\n",
    "model.save_pretrained(\"./deepseek-code-doc-lora\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ ëª¨ë¸ ë¡œë“œ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# LoRA ì ìš©ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./deepseek-code-doc-lora\")\n",
    "\n",
    "# LoRA ë³‘í•© (LoRA ê°€ì¤‘ì¹˜ë¥¼ ì›ë³¸ ëª¨ë¸ì— í•©ì¹¨)\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ (ì´ì œ ì¼ë°˜ ëª¨ë¸ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥)\n",
    "merged_model.save_pretrained(\"./deepseek-code-doc-merged\")\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (LoRA ì ìš© ì „ ì›ë³¸ ëª¨ë¸)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, torch_dtype=torch.float16)\n",
    "\n",
    "# LoRA ê°€ì¤‘ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¤ê¸° (ë””ë ‰í„°ë¦¬ì— ë‹¤ë¥¸ íŒŒì¼ì´ ìˆì–´ë„ ë¬¸ì œì—†ìŒ)\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./deepseek-code-doc-lora\", torch_dtype=torch.float16) \n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator ì¤€ë¹„ (mixed precision ì ìš©)\n",
    "lora_model = accelerator.prepare(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerator = Accelerator()\n",
    "#lora_model = accelerator.prepare(lora_model)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ ì ìš©\n",
    "lora_model.config.use_cache = False  # ìºì‹œ ë¹„í™œì„±í™” â†’ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "lora_model.gradient_checkpointing_enable()  # ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” â†’ GPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ (dispatch_model ì—†ì´ ë°”ë¡œ ì‚¬ìš©)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,  # âœ… í† í° ìˆ˜ ì ˆì•½\n",
    "    device_map='auto',   # âœ… ì—¬ëŸ¬ GPU ìë™ ë¶„ë°°\n",
    "    #offload_folder=\"./offload\",  # CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    #offload_state_dict=True  # ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì˜¤í”„ë¡œë“œ\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "document = f\"\"\"\n",
    "if ((pMsg->message == WM_KEYDOWN &&\n",
    "\t\t\t(pMsg->wParam == VK_LEFT || pMsg->wParam == VK_RIGHT ||\n",
    "\t\t\t\tpMsg->wParam == VK_UP || pMsg->wParam == VK_DOWN)))\n",
    "ì´ ì½”ë“œë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš” ê·¸ê±¸ ê·¸ë¦¬ê³  ì½”ë“œë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    1. ì½”ë“œë³´ì—¬ì¤˜\n",
    "    2. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´\n",
    "    Document:\n",
    "    {document}\n",
    "    \"\"\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# ìƒì„±\n",
    "with torch.no_grad():\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì˜¨ë„ ì„¤ì •\n",
    "        top_k=50,  # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ top k ê°œì˜ ë‹¨ì–´ë§Œ ê³ ë ¤\n",
    "        top_p=0.9,  # ëˆ„ì  í™•ë¥ ì´ 90%ì¸ ë‹¨ì–´ë“¤ë§Œ ê³ ë ¤\n",
    "        add_special_tokens=True,\n",
    "        eos_token_id=[  \n",
    "            pipe.tokenizer.eos_token_id,\n",
    "            pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
