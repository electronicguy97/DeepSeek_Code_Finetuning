{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "#from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.amp import autocast, GradScaler\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # Î©îÎ™®Î¶¨ Ï°∞Í∞Å Î∞©ÏßÄÏßÄ\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_MODEL = \"./DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"code_search_net\", \"python\")  # Ïñ∏Ïñ¥ ÏÑ†ÌÉù Í∞ÄÎä• (ex: python, java)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:784: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.62s/it]\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    #target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Í∞ÄÏ§ëÏπò Ï†ÅÏö©Ìï† Î†àÏù¥Ïñ¥\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],  # Í∞ÄÏ§ëÏπò Ï†ÅÏö©Ìï† Î†àÏù¥Ïñ¥\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4bit ÏñëÏûêÌôî ÏÑ§Ï†ï - QLoRAÎ°ú Ìï¥Ïïº Ìï®\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ìå®Îî© ÌÜ†ÌÅ∞ ÏÑ§Ï†ï\n",
    "\n",
    "# 4-bit ÏñëÏûêÌôîÎêú Î™®Îç∏ Î°úÎìú\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    #device_map=\"sequential\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # 4-bit ÏÑ§Ï†ï Ï†ÅÏö©\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn ÏóêÎü¨\n",
    "model.enable_input_require_grads() # get_input_embeddings().weight.requires_grad = True \n",
    "# LoRA Ï†ÅÏö©\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# LoRAÍ∞Ä Ï†ÅÏö©Îêú Î†àÏù¥Ïñ¥Îßå ÌïôÏäµ (Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ freeze)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23107/23107 [00:07<00:00, 2889.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Ìï®Ïàò ÏΩîÎìúÏôÄ DocstringÏùÑ ÌïòÎÇòÏùò ÏûÖÎ†•ÏúºÎ°ú Í≤∞Ìï©\n",
    "    combined_texts = [\n",
    "        f\"{doc}\\n\\n{code}\" for doc, code in zip(examples[\"func_documentation_string\"], examples[\"func_code_string\"])\n",
    "    ]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        combined_texts,  \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # ‚úÖ `torch.tensor()` ÏÇ¨Ïö©\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokenize_function(examples):\\n    tokenized = tokenizer(\\n        examples[\"func_code_string\"], \\n        truncation=True, \\n        padding=\"max_length\", \\n        max_length=128\\n    )\\n    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # ‚úÖ `torch.tensor()` ÏÇ¨Ïö©\\n    return tokenized\\n\\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"func_code_string\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # ‚úÖ `torch.tensor()` ÏÇ¨Ïö©\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "'''\n",
    "\n",
    "#tokenized_datasets.set_format(type=\"torch\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-code-doc-lora\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_checkpointing=False,  # T: GPU Î©îÎ™®Î¶¨ Ï†àÏïΩ\n",
    "    gradient_accumulation_steps=2,  # ÏûëÏùÄ Î∞∞Ïπò ÌÅ¨Í∏∞ Î≥¥ÏôÑ\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./code_logs\",\n",
    "    fp16=True,  # 16-bit Ïó∞ÏÇ∞\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\deepseek\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53770' max='103044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 53770/103044 62:24:21 < 57:11:24, 0.24 it/s, Epoch 0.52/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Í∞ÄÏ§ëÏπòÎßå Ï†ÄÏû•\n",
    "model.save_pretrained(\"./deepseek-code-doc-lora\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏõêÎ≥∏ Î™®Îç∏ Î°úÎìú\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# LoRA Ï†ÅÏö©Îêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./deepseek-code-doc-lora\")\n",
    "\n",
    "# LoRA Î≥ëÌï© (LoRA Í∞ÄÏ§ëÏπòÎ•º ÏõêÎ≥∏ Î™®Îç∏Ïóê Ìï©Ïπ®)\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Î≥ëÌï©Îêú Î™®Îç∏ Ï†ÄÏû• (Ïù¥Ï†ú ÏùºÎ∞ò Î™®Îç∏Ï≤òÎüº ÏÇ¨Ïö© Í∞ÄÎä•)\n",
    "merged_model.save_pretrained(\"./deepseek-code-doc-merged\")\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Í∏∞Î≥∏ Î™®Îç∏ Î°úÎìú (LoRA Ï†ÅÏö© Ï†Ñ ÏõêÎ≥∏ Î™®Îç∏)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, torch_dtype=torch.float16)\n",
    "\n",
    "# LoRA Í∞ÄÏ§ëÏπòÎßå Î∂àÎü¨Ïò§Í∏∞ (ÎîîÎ†âÌÑ∞Î¶¨Ïóê Îã§Î•∏ ÌååÏùºÏù¥ ÏûàÏñ¥ÎèÑ Î¨∏Ï†úÏóÜÏùå)\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./deepseek-code-doc-lora\", torch_dtype=torch.float16) \n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator Ï§ÄÎπÑ (mixed precision Ï†ÅÏö©)\n",
    "lora_model = accelerator.prepare(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerator = Accelerator()\n",
    "#lora_model = accelerator.prepare(lora_model)\n",
    "\n",
    "# Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏòµÏÖò Ï†ÅÏö©\n",
    "lora_model.config.use_cache = False  # Ï∫êÏãú ÎπÑÌôúÏÑ±Ìôî ‚Üí Î©îÎ™®Î¶¨ Ï†àÏïΩ\n",
    "lora_model.gradient_checkpointing_enable()  # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌôúÏÑ±Ìôî ‚Üí GPU Î©îÎ™®Î¶¨ Ï†àÏïΩ\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ ÏÉùÏÑ± ÌååÏù¥ÌîÑÎùºÏù∏ (dispatch_model ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö©)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,  # ‚úÖ ÌÜ†ÌÅ∞ Ïàò Ï†àÏïΩ\n",
    "    device_map='auto',   # ‚úÖ Ïó¨Îü¨ GPU ÏûêÎèô Î∂ÑÎ∞∞\n",
    "    #offload_folder=\"./offload\",  # CPUÎ°ú Ïò§ÌîÑÎ°úÎìúÌïòÏó¨ GPU Î©îÎ™®Î¶¨ Ï†àÏïΩ\n",
    "    #offload_state_dict=True  # ÏÉÅÌÉú ÎîïÏÖîÎÑàÎ¶¨ Ïò§ÌîÑÎ°úÎìú\n",
    ")\n",
    "\n",
    "# ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï\n",
    "document = f\"\"\"\n",
    "if ((pMsg->message == WM_KEYDOWN &&\n",
    "\t\t\t(pMsg->wParam == VK_LEFT || pMsg->wParam == VK_RIGHT ||\n",
    "\t\t\t\tpMsg->wParam == VK_UP || pMsg->wParam == VK_DOWN)))\n",
    "Ïù¥ ÏΩîÎìúÎ•º Îçî Ìö®Ïú®Ï†ÅÏúºÎ°ú Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî Í∑∏Í±∏ Í∑∏Î¶¨Í≥† ÏΩîÎìúÎ°ú ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    1. ÏΩîÎìúÎ≥¥Ïó¨Ï§ò\n",
    "    2. Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú ÏÑ§Î™ÖÌï¥\n",
    "    Document:\n",
    "    {document}\n",
    "    \"\"\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# ÏÉùÏÑ±\n",
    "with torch.no_grad():\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # ÏÉòÌîåÎßÅÏùò Îã§ÏñëÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌïú Ïò®ÎèÑ ÏÑ§Ï†ï\n",
    "        top_k=50,  # Í∞ÄÏû• ÎÜíÏùÄ ÌôïÎ•†ÏùÑ Í∞ÄÏßÑ top k Í∞úÏùò Îã®Ïñ¥Îßå Í≥†Î†§\n",
    "        top_p=0.9,  # ÎàÑÏ†Å ÌôïÎ•†Ïù¥ 90%Ïù∏ Îã®Ïñ¥Îì§Îßå Í≥†Î†§\n",
    "        add_special_tokens=True,\n",
    "        eos_token_id=[  \n",
    "            pipe.tokenizer.eos_token_id,\n",
    "            pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
